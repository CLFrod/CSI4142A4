{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI 4142 - Introduction to Data Science\n",
    "# Assignment 4: Unsupervised Learning, Clustering and Recommendations.\n",
    "\n",
    "Shacha Parker (300235525)\\\n",
    "Callum Frodsham and (300199446)\\\n",
    "Group 79\n",
    "\n",
    "### Setup Instructions To Reproduce this Notebook:\n",
    "(Step 1 Optional)\n",
    "1. Create a virtual python environment in the project directory (if you want) for all of the packages required:  \n",
    "``` \n",
    "python -m venv .venv\n",
    "```\n",
    "To enter the virutal environment: \n",
    "```\n",
    ".venv/Scripts/activate.ps1 # on windows\n",
    "source .venv/bin/activate # on mac/linux\n",
    "```\n",
    "2. Download all of the required packages (run in cmd/shell of choice):\n",
    "```\n",
    "pip install jupyter\n",
    "pip install ipykernel\n",
    "pip install pandas\n",
    "pip install numpy\n",
    "```\n",
    "3. VSCode: Ensure you have the correct python kernel selected!\n",
    "<br> \n",
    "If you are using a virtual environment, make sure to select the python interpreter for that virtual environment otherwise this will not work! If you have everything done globally, then just make sure the correct python kernel you are using is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset: </h1>\n",
    "Author: Rounak Banik\n",
    "<br>\n",
    "Purpose: The purpose of this dataset is to provide insight on a largage amount of movie data comprised of 45,000 movies released on or before July 2017 and 26 million accompanying ratings from 270,000 users of the GroupLens website. \n",
    "<br>\n",
    "Shape: This dataset is composed of 24 columns, 45466 rows.\n",
    "<br><br>\n",
    "Link: <a href=\"https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset\"> The Movies Dataset</a>\n",
    "<br>\n",
    "\n",
    "Note: The \"homepage\", \"poster_path\" and \"video\" features will be omitted as they serve no purpose in notebook. (since we don't have access to any of the files they're referencing.)\n",
    "<h3>Dataset Feature List: </h3>\n",
    "movies_metadata.csv:\n",
    "<ol>\n",
    "    <li>adult:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: Indicates if the movie is X-rated or not.\n",
    "    </li>\n",
    "    <li>belongs_to_collection:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: Stringified dictionary that indicates which collection of films the movie belongs to. Empty if no collection.\n",
    "    </li>\n",
    "    <li>budget:\n",
    "    <br>\n",
    "    Feature Type: Numerical\n",
    "    <br>\n",
    "    Description: The budget of the film in dollars (USD). 0 if budget is unknown.\n",
    "    </li>\n",
    "    <li>genres:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: Stringified list of dictionaries, that include the films genre(s).\n",
    "    </li>\n",
    "    <li>original_language:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: The film's language of origin.\n",
    "    </li>\n",
    "    <li>original_title:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: The original title of the movie on release.\n",
    "    </li>\n",
    "    <li>overview:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: A brief description of the movie.\n",
    "    </li>\n",
    "    <li>popularity:\n",
    "    <br>\n",
    "    Feature Type: Numerical\n",
    "    <br>\n",
    "    Description: The popularity score as assigned by TMDB.\n",
    "    </li>\n",
    "    <li>production_companies:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: Stringified list of production companies involved in creating the movie.\n",
    "    </li>\n",
    "    <li>production_countries:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: Stringified list of countries where the film was shot in.\n",
    "    </li>\n",
    "    <li>release_data:\n",
    "    <br>\n",
    "    Feature Type: Numerical\n",
    "    <br>\n",
    "    Description: The release date of the movie.\n",
    "    </li>\n",
    "    <li>revenue:\n",
    "    <br>\n",
    "    Feature Type: Numerical\n",
    "    <br>\n",
    "    Description: Total revenue of the film in dollars.\n",
    "    </li>\n",
    "    <li>runtime:\n",
    "    <br>\n",
    "    Feature Type: Numerical\n",
    "    <br>\n",
    "    Description: The runtime of the film in minutes.\n",
    "    </li>\n",
    "    <li>spoken_languages:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: Stringified list of dictionaries of the languages spoken in the film.\n",
    "    </li>\n",
    "    <li>status:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: The release status of the film, with categories: 'Released', 'Rumored', 'Post Production', 'In Production', 'Planned', 'Canceled'\n",
    "    </li>\n",
    "    <li>Tagline:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: The tagline of the movie.\n",
    "    </li>\n",
    "    <li>title:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: The title of the movie.\n",
    "    </li>\n",
    "    <li>vote_average:\n",
    "    <br>\n",
    "    Feature Type: Numerical\n",
    "    <br>\n",
    "    Description: The average rating of the movie.\n",
    "    </li>\n",
    "    <li>vote_count:\n",
    "    <br>\n",
    "    Feature Type: Numerical\n",
    "    <br>\n",
    "    Description: The number number of votes by users as counted by TMDB.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import Levenshtein as le\n",
    "import heapq\n",
    "import ast\n",
    "\n",
    "# load the dataset\n",
    "dataset = pd.read_csv(\"movies_metadata.csv\")\n",
    "\n",
    "# drop the unused columns mentioned above:\n",
    "dataset.drop(columns=['homepage', 'poster_path', 'video'], inplace=True)\n",
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the general info of the dataset\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which columns have missing values:\n",
    "missing_values = dataset.isna().sum()\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Original language data imputation: </h5>\n",
    "The original language is missing 11 data points, however, we can manually impute using data from rotten tomatoes.\n",
    "Since rotten tomatoes does not have an easily accessible api, and using an API for only 11 data points would be a little silly, we shall manually get the data for each point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the or language\n",
    "missing_language = dataset['original_language'].isna()\n",
    "\n",
    "# get the indices of the missing vals\n",
    "print(dataset[missing_language]['title'])  \n",
    "\n",
    "missing_language_input_vals = [ \"en\",\n",
    "                                \"en\",\n",
    "                                \"en\",\n",
    "                                \"en\",\n",
    "                                \"cs\",\n",
    "                                \"en\",\n",
    "                                \"zxx\", # silent film ISO code\n",
    "                                \"en\",\n",
    "                                \"en\",\n",
    "                                \"en\",\n",
    "                                \"zxx\" # also a silent film.\n",
    "                                ]\n",
    "# get the indices of the missing values.\n",
    "missing_language_indices = list(dataset[missing_language].index)\n",
    "\n",
    "# fill in the values:\n",
    "for i, row_num  in enumerate(missing_language_indices):\n",
    "    dataset.at[row_num, 'original_language'] = missing_language_input_vals[i]\n",
    "\n",
    "print(f\"New NaN Value count for the original language feature: {dataset['original_language'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fill the 6 null title rows with their \"original_title\" counterpart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the or language\n",
    "missing_titles = dataset['title'].isna()\n",
    "\n",
    "# lets see which titles are valid: \n",
    "dataset[missing_titles]['original_title']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that some of the original title values are not valid, and fail the format checking. (because it also just so happens that these are the only 3 values that fail the format check of the 'original_title' feature) Thus, we will only update the 'title' feature for rows 19729, 29502, and 35586, and remove the other 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the missing titles.\n",
    "remove_titles = [35587, 29503,19730]\n",
    "dataset.drop(index=remove_titles, inplace=True)\n",
    "\n",
    "# get new missing_titles\n",
    "missing_titles = dataset['title'].isna()\n",
    "dataset[missing_titles]['original_title']\n",
    "\n",
    "# update the other 3 missing title values using the original title.\n",
    "dataset.loc[missing_titles, 'title'] = dataset.loc[missing_titles, 'original_title'] \n",
    "\n",
    "# show the fixed titles!\n",
    "dataset[missing_titles]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row Removal Rationalization: <br>\n",
    "Since the dataset has 45000 some rows, we will be able to remove a small amount of rows without affecting the quality of the data. \n",
    "We will be removing all NULL rows in these features: popularity, production_countries, production_companies, release_date, status, vote_average, vote_count, and runtime.\n",
    "\n",
    "Runtime has a lot of missing values, specifically, 242. These will be removed, but overview will not because that would include 1/45th the dataset approximately, and sometimes movies don't have a succint overview. Thus, all of the missing overview values will be kept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove them all in one fell swoop:\n",
    "dataset.dropna(subset=['popularity', 'production_countries', 'production_companies', 'release_date', 'status', 'vote_average', 'vote_count','runtime', 'imdb_id'], inplace=True)\n",
    "dataset.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# budget has the wrong datatype, convert it to int.\n",
    "dataset['budget'] = pd.to_numeric(dataset['budget'], errors='coerce')\n",
    "\n",
    "sum_budet = (dataset['budget'] == 0).sum()\n",
    "sum_revenue = (dataset['revenue'] == 0).sum()\n",
    "print(sum_revenue)\n",
    "print(sum_budet)\n",
    "\n",
    "# ensure IDs are all numeric as well\n",
    "dataset['id'] = pd.to_numeric(dataset['id'], errors='coerce')\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study 1: Similarity Measures \n",
    "Attribute subsets chosen: title, budget, popularity, vote_count, genre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Subset: title\n",
    "Similarity measure: Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_title = 'Star Wars'\n",
    "# going to use a priority queue to get the top 10:\n",
    "pq = []\n",
    "for idx, row in dataset.iterrows():\n",
    "    title = row['title']\n",
    "    if title == chosen_title:\n",
    "        continue\n",
    "    word_distance = le.distance(chosen_title, title)\n",
    "    if len(pq) < 10:\n",
    "        heapq.heappush(pq, (-word_distance, [title, idx]))\n",
    "    else:\n",
    "        heapq.heappushpop(pq, (-word_distance, [title, idx]))\n",
    "\n",
    "# Switch the values from negative to positive:\n",
    "for i in range(0, len(pq)):\n",
    "    pq[i] = (-pq[i][0], pq[i][1])\n",
    "# sort them from least to greatest\n",
    "pq.sort(key=lambda x: x[0])\n",
    "\n",
    "print(f\"Top 10 titles similar to: {chosen_title}\")\n",
    "title_indices = []\n",
    "for idx, item in enumerate(pq):\n",
    "    title_indices.append(item[1][1])\n",
    "    print(f'{idx +1}. title: {item[1][0]}, distance:{item[0]}')\n",
    "dataset.loc[title_indices, ['title','runtime', 'release_date', 'popularity', 'genres']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Subset: budget\n",
    "Similarity measure: Manhattan Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen movie is Interview with the Vampire, and the budget can be seen below:\n",
    "chosen_title = 'Interview with the Vampire'\n",
    "chosen_movie_budget = (dataset.loc[dataset['title'] == chosen_title, 'budget']).item()\n",
    "# going to use a priority queue to get the top 10:\n",
    "pq = []\n",
    "\n",
    "for idx, row in dataset.iterrows():\n",
    "    cur_movie_budget = row['budget']\n",
    "    if cur_movie_budget == chosen_movie_budget:\n",
    "        continue\n",
    "    budget_distance = abs(chosen_movie_budget - cur_movie_budget)\n",
    "    # budget distance is always negative to ensure the heapq \"kicks out\" larger values.\n",
    "    if len(pq) < 10:\n",
    "        heapq.heappush(pq, (-budget_distance, [row['title'], cur_movie_budget, idx]))\n",
    "    else:\n",
    "        heapq.heappushpop(pq, (-budget_distance, [row['title'], cur_movie_budget, idx]))\n",
    "\n",
    "# reverse the negative value\n",
    "for i in range(0, len(pq)):\n",
    "    pq[i] = (-pq[i][0], pq[i][1])\n",
    "pq.sort(key = lambda x: x[0])\n",
    "budget_indices = []\n",
    "# get the indices and print out the values:\n",
    "for idx, item in enumerate(pq):\n",
    "    budget_indices.append(item[1][-1])\n",
    "    print(f'{idx +1}. title: {item[1][0]}, budget: {item[1][1]}, budget distance: {item[0]}')\n",
    "dataset.loc[budget_indices, ['title','runtime', 'release_date', 'popularity', 'genres']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Subset: popularity\n",
    "Similarity measure: Euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth Subset: vote_count \n",
    "Similarity measure: Hamming\n",
    "\n",
    "Steps: Find the larget integer, and then pick the maximum bit length to use for comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth Subset: genre\n",
    "Similarity measure: Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that converts the genre format into a list of genres.\n",
    "def convert_genre_str_to_list(genre_list_str:str):\n",
    "    dict_list = ast.literal_eval(genre_list_str)\n",
    "    genre_list = []\n",
    "    for item in dict_list:\n",
    "        genre_list.append(item['name'])\n",
    "    return genre_list\n",
    "# since sci-kit wants us to encode our genres: lets not do that and make our own function\n",
    "def jaccard_similarity(genres_a, genres_b):\n",
    "    set_genres_a = set(genres_a)\n",
    "    set_genres_b = set(genres_b)\n",
    "    intersection_of_sets = set_genres_a & set_genres_b\n",
    "    union_of_sets = set_genres_a | set_genres_b\n",
    "\n",
    "    if len(union_of_sets) == 0:\n",
    "        return 0\n",
    "    return len(intersection_of_sets) / len(union_of_sets)     \n",
    "chosen_title = 'Astérix and Obélix: God Save Britannia'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study 2: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "<ul>\n",
    "<li>\n",
    "<a href=\"https://www.analyticsvidhya.com/blog/2024/02/ways-to-convert-string-to-a-list-in-python/\">Parsing StringList using ast</a>\n",
    "</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
