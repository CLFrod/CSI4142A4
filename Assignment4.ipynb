{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI 4142 - Introduction to Data Science\n",
    "# Assignment 4: Unsupervised Learning, Clustering and Recommendations.\n",
    "\n",
    "Shacha Parker (300235525)\\\n",
    "Callum Frodsham and (300199446)\\\n",
    "Group 79\n",
    "\n",
    "### Setup Instructions To Reproduce this Notebook:\n",
    "(Step 1 Optional)\n",
    "1. Create a virtual python environment in the project directory (if you want) for all of the packages required:  \n",
    "``` \n",
    "python -m venv .venv\n",
    "```\n",
    "To enter the virutal environment: \n",
    "```\n",
    ".venv/Scripts/activate.ps1 # on windows\n",
    "source .venv/bin/activate # on mac/linux\n",
    "```\n",
    "2. Download all of the required packages (run in cmd/shell of choice):\n",
    "```\n",
    "pip install jupyter\n",
    "pip install ipykernel\n",
    "pip install pandas\n",
    "pip install numpy\n",
    "pip install scikit-learn\n",
    "pip install levenshtein\n",
    "pip install seaborn\n",
    "```\n",
    "3. VSCode: Ensure you have the correct python kernel selected!\n",
    "<br> \n",
    "If you are using a virtual environment, make sure to select the python interpreter for that virtual environment otherwise this will not work! If you have everything done globally, then just make sure the correct python kernel you are using is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset: </h1>\n",
    "Author: Rounak Banik\n",
    "<br>\n",
    "Purpose: The purpose of this dataset is to provide insight on a largage amount of movie data comprised of 45,000 movies released on or before July 2017 and 26 million accompanying ratings from 270,000 users of the GroupLens website. \n",
    "<br>\n",
    "Shape: This dataset is composed of 24 columns, 45466 rows.\n",
    "<br><br>\n",
    "Link: <a href=\"https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset\"> The Movies Dataset</a>\n",
    "<br>\n",
    "\n",
    "Note: The \"homepage\", \"poster_path\" and \"video\" features will be omitted as they serve no purpose in notebook. (since we don't have access to any of the files they're referencing.)\n",
    "<h3>Dataset Feature List: </h3>\n",
    "movies_metadata.csv:\n",
    "<ol>\n",
    "    <li>adult:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: Indicates if the movie is X-rated or not.\n",
    "    </li>\n",
    "    <li>belongs_to_collection:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: Stringified dictionary that indicates which collection of films the movie belongs to. Empty if no collection.\n",
    "    </li>\n",
    "    <li>budget:\n",
    "    <br>\n",
    "    Feature Type: Numerical\n",
    "    <br>\n",
    "    Description: The budget of the film in dollars (USD). 0 if budget is unknown.\n",
    "    </li>\n",
    "    <li>genres:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: Stringified list of dictionaries, that include the films genre(s).\n",
    "    </li>\n",
    "    <li>original_language:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: The film's language of origin.\n",
    "    </li>\n",
    "    <li>original_title:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: The original title of the movie on release.\n",
    "    </li>\n",
    "    <li>overview:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: A brief description of the movie.\n",
    "    </li>\n",
    "    <li>popularity:\n",
    "    <br>\n",
    "    Feature Type: Numerical\n",
    "    <br>\n",
    "    Description: The popularity score as assigned by TMDB.\n",
    "    </li>\n",
    "    <li>production_companies:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: Stringified list of production companies involved in creating the movie.\n",
    "    </li>\n",
    "    <li>production_countries:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: Stringified list of countries where the film was shot in.\n",
    "    </li>\n",
    "    <li>release_data:\n",
    "    <br>\n",
    "    Feature Type: Numerical\n",
    "    <br>\n",
    "    Description: The release date of the movie.\n",
    "    </li>\n",
    "    <li>revenue:\n",
    "    <br>\n",
    "    Feature Type: Numerical\n",
    "    <br>\n",
    "    Description: Total revenue of the film in dollars.\n",
    "    </li>\n",
    "    <li>runtime:\n",
    "    <br>\n",
    "    Feature Type: Numerical\n",
    "    <br>\n",
    "    Description: The runtime of the film in minutes.\n",
    "    </li>\n",
    "    <li>spoken_languages:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: Stringified list of dictionaries of the languages spoken in the film.\n",
    "    </li>\n",
    "    <li>status:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: The release status of the film, with categories: 'Released', 'Rumored', 'Post Production', 'In Production', 'Planned', 'Canceled'\n",
    "    </li>\n",
    "    <li>Tagline:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: The tagline of the movie.\n",
    "    </li>\n",
    "    <li>title:\n",
    "    <br>\n",
    "    Feature Type: Categorical\n",
    "    <br>\n",
    "    Description: The title of the movie.\n",
    "    </li>\n",
    "    <li>vote_average:\n",
    "    <br>\n",
    "    Feature Type: Numerical\n",
    "    <br>\n",
    "    Description: The average rating of the movie.\n",
    "    </li>\n",
    "    <li>vote_count:\n",
    "    <br>\n",
    "    Feature Type: Numerical\n",
    "    <br>\n",
    "    Description: The number number of votes by users as counted by TMDB.\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import Levenshtein as le\n",
    "import heapq\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the dataset\n",
    "dataset = pd.read_csv(\"https://raw.githubusercontent.com/CLFrod/CSI4142A4/refs/heads/main/movies_metadata.csv\")\n",
    "\n",
    "# drop the unused columns mentioned above:\n",
    "dataset.drop(columns=['homepage', 'poster_path', 'video'], inplace=True)\n",
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the general info of the dataset\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which columns have missing values:\n",
    "missing_values = dataset.isna().sum()\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Original language data imputation: </h5>\n",
    "The original language is missing 11 data points, however, we can manually impute using data from rotten tomatoes.\n",
    "Since rotten tomatoes does not have an easily accessible api, and using an API for only 11 data points would be a little silly, we shall manually get the data for each point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the or language\n",
    "missing_language = dataset['original_language'].isna()\n",
    "\n",
    "# get the indices of the missing vals\n",
    "print(dataset[missing_language]['title'])  \n",
    "\n",
    "missing_language_input_vals = [ \"en\",\n",
    "                                \"en\",\n",
    "                                \"en\",\n",
    "                                \"en\",\n",
    "                                \"cs\",\n",
    "                                \"en\",\n",
    "                                \"zxx\", # silent film ISO code\n",
    "                                \"en\",\n",
    "                                \"en\",\n",
    "                                \"en\",\n",
    "                                \"zxx\" # also a silent film.\n",
    "                                ]\n",
    "# get the indices of the missing values.\n",
    "missing_language_indices = list(dataset[missing_language].index)\n",
    "\n",
    "# fill in the values:\n",
    "for i, row_num  in enumerate(missing_language_indices):\n",
    "    dataset.at[row_num, 'original_language'] = missing_language_input_vals[i]\n",
    "\n",
    "print(f\"New NaN Value count for the original language feature: {dataset['original_language'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fill the 6 null title rows with their \"original_title\" counterpart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the or language\n",
    "missing_titles = dataset['title'].isna()\n",
    "\n",
    "# lets see which titles are valid: \n",
    "dataset[missing_titles]['original_title']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that some of the original title values are not valid, and fail the format checking. (because it also just so happens that these are the only 3 values that fail the format check of the 'original_title' feature) Thus, we will only update the 'title' feature for rows 19729, 29502, and 35586, and remove the other 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the missing titles.\n",
    "remove_titles = [35587, 29503,19730]\n",
    "dataset.drop(index=remove_titles, inplace=True)\n",
    "\n",
    "# get new missing_titles\n",
    "missing_titles = dataset['title'].isna()\n",
    "dataset[missing_titles]['original_title']\n",
    "\n",
    "# update the other 3 missing title values using the original title.\n",
    "dataset.loc[missing_titles, 'title'] = dataset.loc[missing_titles, 'original_title'] \n",
    "\n",
    "# show the fixed titles!\n",
    "dataset[missing_titles]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row Removal Rationalization: <br>\n",
    "Since the dataset has 45000 some rows, we will be able to remove a small amount of rows without affecting the quality of the data. \n",
    "We will be removing all NULL rows in these features: popularity, production_countries, production_companies, release_date, status, vote_average, vote_count, and runtime.\n",
    "\n",
    "Runtime has a lot of missing values, specifically, 242. These will be removed, but overview will not because that would include 1/45th the dataset approximately, and sometimes movies don't have a succint overview. Thus, all of the missing overview values will be kept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove them all in one fell swoop:\n",
    "dataset.dropna(subset=['popularity', 'production_countries', 'production_companies', 'release_date', 'status', 'vote_average', 'vote_count','runtime', 'imdb_id'], inplace=True)\n",
    "dataset.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# budget has the wrong datatype, convert it to int.\n",
    "dataset['budget'] = pd.to_numeric(dataset['budget'], errors='coerce')\n",
    "\n",
    "sum_budet = (dataset['budget'] == 0).sum()\n",
    "sum_revenue = (dataset['revenue'] == 0).sum()\n",
    "print(sum_revenue)\n",
    "print(sum_budet)\n",
    "\n",
    "# ensure IDs are all numeric as well\n",
    "dataset['id'] = pd.to_numeric(dataset['id'], errors='coerce')\n",
    "\n",
    "# ensure popularity values are all numeric:\n",
    "dataset['popularity'] = pd.to_numeric(dataset['popularity'], errors='coerce')\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study 1: Similarity Measures \n",
    "Attribute subsets chosen: title, budget, popularity, vote_count, genre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Subset: title\n",
    "Similarity measure: Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_title = 'Star Wars'\n",
    "# going to use a priority queue to get the top 10:\n",
    "pq = []\n",
    "for idx, row in dataset.iterrows():\n",
    "    title = row['title']\n",
    "    if title == chosen_title:\n",
    "        continue\n",
    "    word_distance = le.distance(chosen_title, title)\n",
    "    if len(pq) < 10:\n",
    "        heapq.heappush(pq, (-word_distance, [title, idx]))\n",
    "    else:\n",
    "        heapq.heappushpop(pq, (-word_distance, [title, idx]))\n",
    "\n",
    "# Switch the values from negative to positive:\n",
    "for i in range(0, len(pq)):\n",
    "    pq[i] = (-pq[i][0], pq[i][1])\n",
    "# sort them from least to greatest\n",
    "pq.sort(key=lambda x: x[0])\n",
    "\n",
    "print(f\"Top 10 titles similar to: {chosen_title}\")\n",
    "title_indices = []\n",
    "for idx, item in enumerate(pq):\n",
    "    title_indices.append(item[1][1])\n",
    "    print(f'{idx +1}. title: {item[1][0]}, distance:{item[0]}')\n",
    "dataset.loc[title_indices, ['title','runtime', 'release_date', 'popularity', 'genres']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Subset: budget\n",
    "Similarity measure: Manhattan Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen movie is Interview with the Vampire, and the budget can be seen below:\n",
    "chosen_title = 'Interview with the Vampire'\n",
    "chosen_movie_budget = (dataset.loc[dataset['title'] == chosen_title, 'budget']).item()\n",
    "# going to use a priority queue to get the top 10:\n",
    "pq = []\n",
    "for idx, row in dataset.iterrows():\n",
    "    cur_movie_budget = row['budget']\n",
    "    if cur_movie_budget == chosen_movie_budget:\n",
    "        continue\n",
    "    budget_distance = abs(chosen_movie_budget - cur_movie_budget)\n",
    "    # budget distance is always negative to ensure the heapq \"kicks out\" larger values.\n",
    "    if len(pq) < 10:\n",
    "        heapq.heappush(pq, (-budget_distance, [row['title'], cur_movie_budget, idx]))\n",
    "    else:\n",
    "        heapq.heappushpop(pq, (-budget_distance, [row['title'], cur_movie_budget, idx]))\n",
    "\n",
    "# reverse the negative value\n",
    "for i in range(0, len(pq)):\n",
    "    pq[i] = (-pq[i][0], pq[i][1])\n",
    "pq.sort(key = lambda x: x[0])\n",
    "budget_indices = []\n",
    "# get the indices and print out the values:\n",
    "for idx, item in enumerate(pq):\n",
    "    budget_indices.append(item[1][-1])\n",
    "    print(f'{idx +1}. title: {item[1][0]}, budget: {item[1][1]}, budget distance: {item[0]}')\n",
    "dataset.loc[budget_indices, ['title','runtime', 'release_date', 'popularity', 'genres']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Subset: popularity\n",
    "Similarity measure: Manhattan Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # chosen movie is Interview with the Vampire, and the budget can be seen below:\n",
    "chosen_title = 'Jumanji'\n",
    "chosen_movie_popularity = (dataset.loc[dataset['title'] == chosen_title, 'popularity']).item()\n",
    "print(f'Popularity of {chosen_title}: {chosen_movie_popularity}')\n",
    "# going to use a priority queue to get the top 10:\n",
    "pq = []\n",
    "\n",
    "for idx, row in dataset.iterrows():\n",
    "    cur_movie_popularity = row['popularity']\n",
    "    if cur_movie_popularity == chosen_movie_popularity:\n",
    "        continue\n",
    "    popularity_distance = abs(chosen_movie_popularity - cur_movie_popularity)\n",
    "    # budget distance is always negative to ensure the heapq \"kicks out\" larger values.\n",
    "    if len(pq) < 10:\n",
    "        heapq.heappush(pq, (-popularity_distance, [row['title'], cur_movie_popularity, idx]))\n",
    "    else:\n",
    "        heapq.heappushpop(pq, (-popularity_distance, [row['title'], cur_movie_popularity, idx]))\n",
    "\n",
    "# reverse the negative value\n",
    "for i in range(0, len(pq)):\n",
    "    pq[i] = (-pq[i][0], pq[i][1])\n",
    "pq.sort(key = lambda x: x[0])\n",
    "popularity_indices = []\n",
    "# get the indices and print out the values:\n",
    "for idx, item in enumerate(pq):\n",
    "    popularity_indices.append(item[1][-1])\n",
    "    print(f'{idx +1}. title: {item[1][0]}, popularity: {item[1][1]}, popularity distance: {item[0]}')\n",
    "dataset.loc[popularity_indices, ['title','runtime', 'release_date', 'popularity', 'genres']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth Subset: vote_count \n",
    "Similarity measure: Hamming\n",
    "\n",
    "Steps: Find the larget integer, and then pick the maximum bit length to use for comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hamming_similarity_bits(num1:int, num2:int, max_bit_length:int):\n",
    "    str1 = format(num1, f'0{max_bit_length}b')\n",
    "    str2 = format(num2, f'0{max_bit_length}b')\n",
    "    return sum(char1 != char2 for char1, char2 in zip(str1, str2))\n",
    "\n",
    "# get the largest value \n",
    "max_bit_length = int(dataset['vote_count'].max()).bit_length()\n",
    "print(max_bit_length)\n",
    "# so we shall use 14 bits for our hamming similarity measure calculation.\n",
    "chosen_title = 'The Little Mermaid'\n",
    "\n",
    "# there are two little mermaids, get the first one. (the most popular)\n",
    "chosen_movie_info = list(dict((dataset.loc[dataset['title'] == chosen_title, 'vote_count'])).items())\n",
    "chosen_movie_vote_count = int(chosen_movie_info[0][-1])\n",
    "print(f'{chosen_title} Vote Count: {chosen_movie_vote_count}')\n",
    "chosen_movie_index = chosen_movie_info[0][0]\n",
    "\n",
    "pq = []\n",
    "for idx, row in dataset.iterrows():\n",
    "    if idx == chosen_movie_index:\n",
    "        continue\n",
    "    cur_movie_vote_count = int(row['vote_count'])\n",
    "    hamming_similarity = calc_hamming_similarity_bits(chosen_movie_vote_count, cur_movie_vote_count, max_bit_length)\n",
    "\n",
    "    if len(pq) < 10:\n",
    "        heapq.heappush(pq,  (-hamming_similarity, [row['title'], cur_movie_vote_count, idx])) \n",
    "    else:\n",
    "        heapq.heappushpop(pq, (-hamming_similarity, [row['title'], cur_movie_vote_count, idx]))\n",
    "           \n",
    "# reverse the negative value\n",
    "for i in range(0, len(pq)):\n",
    "    pq[i] = (-pq[i][0], pq[i][1])\n",
    "pq.sort(key = lambda x: x[0])\n",
    "\n",
    "vote_count_indices = []\n",
    "for idx, item in enumerate(pq):\n",
    "    vote_count_indices.append(item[1][-1])\n",
    "    print(f'{idx +1}. title: {item[1][0]}, vote_count: {item[1][1]}, vote_count distance: {item[0]}')\n",
    "dataset.loc[vote_count_indices, ['title','runtime', 'release_date', 'popularity', 'genres']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth Subset: genres\n",
    "Similarity measure: Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that converts the genre format into a list of genres.\n",
    "def convert_genre_str_to_list(genre_list_str:str):\n",
    "    dict_list = ast.literal_eval(genre_list_str)\n",
    "    genre_list = []\n",
    "    for item in dict_list:\n",
    "        genre_list.append(item['name'])\n",
    "    return genre_list\n",
    "# since sci-kit wants us to encode our genres: lets not do that and make our own function\n",
    "def jaccard_similarity(genres_a, genres_b):\n",
    "    set_genres_a = set(genres_a)\n",
    "    set_genres_b = set(genres_b)\n",
    "    intersection_of_sets = set_genres_a & set_genres_b\n",
    "    union_of_sets = set_genres_a | set_genres_b\n",
    "    if len(union_of_sets) == 0:\n",
    "        return 0\n",
    "    return len(intersection_of_sets) / len(union_of_sets)     \n",
    "\n",
    "# pick a title: \n",
    "chosen_title = 'Astérix and Obélix: God Save Britannia'\n",
    "# get the genres: (and convert)\n",
    "chosen_movie_genres = convert_genre_str_to_list((dataset.loc[dataset['title'] == chosen_title, 'genres']).item())\n",
    "chosen_movie_index = (dataset.loc[dataset['title'] == chosen_title, 'genres']).index.item()\n",
    "pq = []\n",
    "for idx, row in dataset.iterrows():\n",
    "    cur_movie_genres = convert_genre_str_to_list(row['genres'])\n",
    "    if idx == chosen_movie_index:\n",
    "        continue\n",
    "    genre_similarity = jaccard_similarity(chosen_movie_genres, cur_movie_genres)\n",
    "    # budget distance is always negative to ensure the heapq \"kicks out\" larger values.\n",
    "    if len(pq) < 10:\n",
    "        heapq.heappush(pq, (genre_similarity, [row['title'], cur_movie_genres, idx]))\n",
    "    # remove the first smallest item, and push one with larger similarity:\n",
    "    elif pq[0][0] < genre_similarity:\n",
    "        heapq.heappop(pq)\n",
    "        heapq.heappush(pq, (genre_similarity, [row['title'], cur_movie_genres, idx]))\n",
    "\n",
    "pq.sort(key = lambda x: x[0], reverse=True)\n",
    "\n",
    "genres_indices = []\n",
    "for idx, item in enumerate(pq):\n",
    "    genres_indices.append(item[1][-1])\n",
    "    print(f'{idx +1}. title: {item[1][0]}, genres: {item[1][1]}, jaccard similarity: {item[0]}')\n",
    "dataset.loc[genres_indices, ['title','runtime', 'release_date', 'popularity', 'vote_average']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study 1 Results and Analysis:\n",
    "1. The Levenshtein distance algorithm worked wonderfully to provide similarity values for the textual 'title' attribute. The top 10 values are illustrated under the corresponding code block, but some of the movies seem to be parodies of the chosen 'Star Wars' title. Like 'Beer Wars' for example.\n",
    "2. The manhattan distance for the budget provided movies similar in budget as the selected movie, however just because they have a similar budget does not mean that they have any other similar features.\n",
    "3. Manhattan distance seemed a bit more appropriate for finding possible movies to recommend based on the selected title because it could allow you to find movies that are similarly popular or unpopular. \n",
    "4. Hamming similarity showed inconsistencies in the implemention I chose, where I converted each value into their binary representation, and then appended 0's until they were as long as the largest value found in the vote_count attribute. For example, Moneyball was deemed to be #2 in the list even though it had a vote_count of 1409 compared to the chosen title's (The Little Mermaid) vote count of 1921. This is because one missing binary number can be the only difference, but when it comes to actual value it makes a large difference. Thus, the implementation chosen was flawed for comparing movies that have a similar amount of votes.\n",
    "5. Jaccard Similarity worked perfectly for finding movies that have the same (or similar) genres to the chosen movie. For example, top 10 movies similar to 'Astérix and Obélix: God Save Britannia' using jaccard similarity on the genres attribute we're all movies that have the exact same genres.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study 2: Clustering Algorithms\n",
    "The chosen attributes for each visualization will be, popularity vs budget, vote_count vs budget. We will be comparing the results from two different clustering algorithms on these attributes, specifically, DBSCAN, and KMeans.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Means on Popularity Vs. Budget with k = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# popularity vs budget kmeans separation\n",
    "x_pop_budget = dataset[['popularity', 'budget']].to_numpy(copy=True)\n",
    "\n",
    "# running k means algo\n",
    "k_means = KMeans(n_clusters=2, random_state=23, n_init='auto').fit(x_pop_budget)\n",
    "\n",
    "sns.scatterplot(data=dataset, x=dataset['popularity'], y=dataset['budget'], hue=list(k_means.labels_))\n",
    "plt.title('Popularity Score Vs. Budget (K = 2)')\n",
    "plt.xlabel('Popularity Score')\n",
    "plt.ylabel('Movie Budget (Dollars)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Means on Popularity Vs. Budget with k = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running k means algo, with k =4\n",
    "k_means = KMeans(n_clusters=4, random_state=23, n_init='auto').fit(x_pop_budget)\n",
    "\n",
    "sns.scatterplot(data=dataset, x=dataset['popularity'], y=dataset['budget'], hue=list(k_means.labels_), palette= 'viridis')\n",
    "plt.title('Popularity Score Vs. Budget (K = 4)')\n",
    "plt.xlabel('Popularity Score')\n",
    "plt.ylabel('Movie Budget (Dollars)')\n",
    "plt.legend(labels=['Cluster 1','Cluster 2','Cluster 3','Cluster 4'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Means on Vote Count Vs. Budget with k = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# popularity vs budget kmeans separation\n",
    "x_vote_budget = dataset[['vote_count', 'budget']].to_numpy(copy=True)\n",
    "\n",
    "# running k means algo\n",
    "k_means = KMeans(n_clusters=2, random_state=23, n_init='auto').fit(x_vote_budget)\n",
    "\n",
    "sns.scatterplot(data=dataset, x=dataset['vote_count'], y=dataset['budget'], hue=list(k_means.labels_))\n",
    "plt.title('Vote Count Vs. Budget (K = 2)')\n",
    "plt.xlabel('Vote Count')\n",
    "plt.ylabel('Movie Budget (Dollars)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Means on Vote Count Vs. Budget with k = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running k means algo\n",
    "k_means = KMeans(n_clusters=4, random_state=23, n_init='auto').fit(x_vote_budget)\n",
    "\n",
    "sns.scatterplot(data=dataset, x=dataset['vote_count'], y=dataset['budget'], hue=list(k_means.labels_), palette='viridis')\n",
    "plt.title('Vote Count Vs. Budget (K = 4)')\n",
    "plt.xlabel('Vote Count')\n",
    "plt.ylabel('Movie Budget (Dollars)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN on Popularity Vs. Budget with EPS = 15, and min_samples = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN with EPS = 15, and min samples = 5\n",
    "dbscan_cluster = DBSCAN(eps=15, min_samples=5).fit(x_pop_budget)\n",
    "sns.scatterplot(data=dataset, x=dataset['popularity'], y=dataset['budget'], hue=list(dbscan_cluster.labels_))\n",
    "plt.title('Popularity Score Vs. Budget (DBSCAN)')\n",
    "plt.xlabel('Popularity Score')\n",
    "plt.ylabel('Movie Budget (Dollars)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN on Popularity Vs. Budget with EPS = 50, and min_samples = 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN with EPS = 50, and min samples = 8\n",
    "dbscan_cluster = DBSCAN(eps=50, min_samples=8).fit(x_pop_budget)\n",
    "sns.scatterplot(data=dataset, x=dataset['popularity'], y=dataset['budget'], hue=list(dbscan_cluster.labels_), palette='viridis')\n",
    "plt.title('Popularity Score Vs. Budget (DBSCAN)')\n",
    "plt.xlabel('Popularity Score')\n",
    "plt.ylabel('Movie Budget (Dollars)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN on Vote Count Vs. Budget with EPS = 15, and min_samples = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN with EPS = 15, and min samples = 5\n",
    "dbscan_cluster = DBSCAN(eps=10, min_samples=6).fit(x_vote_budget)\n",
    "sns.scatterplot(data=dataset, x=dataset['vote_count'], y=dataset['budget'], hue=list(dbscan_cluster.labels_), palette='viridis')\n",
    "plt.title('Vote Count Vs. Budget (DBSCAN)')\n",
    "plt.xlabel('Vote Count')\n",
    "plt.ylabel('Movie Budget (Dollars)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN on Vote Count Vs. Budget with EPS = 50, and min_samples = 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN with EPS = 250, and min samples = 8\n",
    "dbscan_cluster = DBSCAN(eps=250, min_samples=6).fit(x_vote_budget)\n",
    "sns.scatterplot(data=dataset, x=dataset['vote_count'], y=dataset['budget'], hue=list(dbscan_cluster.labels_), palette='viridis')\n",
    "plt.title('Vote Count Vs. Budget (DBSCAN)')\n",
    "plt.xlabel('Vote Count')\n",
    "plt.ylabel('Movie Budget (Dollars)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study 2 Results Analysis:\n",
    "1. Popularity Vs. Budget:\n",
    "The KMeans results for k=2 provide two clear clusters, however, not much useful information can be discerened with only two clusters in this case as the data is spread far across the y-axis and includes too many outliers. K=4 is a better parameter choice for this data as each of the 4 clusters include certain bands of the budget. In comparison to DBSCAN, since the data includes many dense subclusters within the main large concentration of data points, the DBSCAN algorithm failed to form any proper clusters on my selected parameter values of eps = 15, 50 , and min_samples 5, 8.\n",
    "\n",
    "2. Vote Count Vs. Budget:\n",
    "The results for the vote count and budget comparison provides largely the same result as the popularity vs budget comparison. The k = 4 was better suited for showing the individual clusters, and their groupings that were mostly separated by budgetary bands. The upper cluster shows us that with a larger budget it is more likely to have a greater number of votes. This is likely due to having a higher advertising budget, and thus more people will have seen the movie, and thus provided their opinion on it. DBSCAN had similar results to the popularity vs. budget comparison, and no meaningful clusters were formed. No matter the parameters, no proper clusters were forming, only mixed smaller clusters that were spread out through the one larger cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "<ul>\n",
    "<li>\n",
    "<a href=\"https://www.analyticsvidhya.com/blog/2024/02/ways-to-convert-string-to-a-list-in-python/\">Parsing StringList using ast</a>\n",
    "</li>\n",
    "<li>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Hamming_distance\">Hamming Similarity Implementations</a>\n",
    "</li>\n",
    "</li>\n",
    "<li>\n",
    "<a href=\"https://pandas.pydata.org/docs/user_guide/index.html\">General Pandas info</a>\n",
    "</li>\n",
    "<li>\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">SkLearn KMeans</a>\n",
    "</li>\n",
    "<li>\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\">SkLearn DBSCAN</a>\n",
    "</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
